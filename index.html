<html>
<head>
  
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
  
<style type="text/css">
* {
  margin: 5 px;
  padding: 0;
}
html {
  overflow-y:scroll; /*keep scrollbar position present in FF at all times*/
  height: 100%;
  background-color: ;/* #white;*/
  background-image: ; /* url("fib.jpg") #ffc38b;*/
}
a {
  color: #240FDF;
}
a:visited {
  color: #240FDF;
}
a:hover {
  color: #ECCF11;
}
p {
  font-family: "Trebuchet MS", Verdana, Helvetica, sans-serif;
}
blockquote {
    display: block;
    margin-top: 1em;
    margin-bottom: 1em;
    margin-left: 80px;
    margin-right: 80px;
}
table {
    border-style: hidden;
}
li{
  margin: 10px 0;
}
name {
    font-family:  "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
    font-size: 47px;
    font-weight: bold;
}
heading {
    font-family:  "Trebuchet MS", Verdana, Helvetica, sans-serif;
    font-size: 21px;
    font-weight: bold;
}
papertitle {
    font-family:  "Trebuchet MS", Verdana, Helvetica, sans-serif;
    font-size: 18px;
    font-weight: bold;
    }
#friendly_1 {
position:absolute;
left:190px;
top:370px;
width:80px;
}
#text {
position:absolute;
left:19px;
top:270px;
width:80px;
}
#footer {
   width:960px;
   margin:0 auto;
}
#footer p {
   line-height:50px; /* must be same as the amount of padding applied to the bottom of the body so text will center vertically */
   text-align:left; /* align right, left or center */
}
</style>


  <title>Liyi Zhang's Page </title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <p> &nbsp </p>
  </head>
  <body>
  <table width="70%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <p>
          <name>Liyi Zhang</name>
        </p>
        <p>
        <img src="zhang-liyi.jpg" width="30%">
        </p>
        <p>
          <a href="https://github.com/zhang-liyi"> GitHub</a> | 
          <a href="https://www.linkedin.com/in/liyi-zhang-1b5041139/"> LinkedIn</a> | 
          <a> Email:</a>  lz2574@columbia.edu 
        </p>
        </td>
       </tr>
      </table>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
        <td width="100%" valign="middle">
        <p>
        I'm doing undergraduate degree at Columbia University, Columbia College' 2021. I'm majoring in Statistics and Applied Mathematics. My research interest lies in the general area of probabilistic Machine Learning. They include the intersection between Deep Learning and probabilistic modeling, and approximate Bayesian inference. I'm also interested in model combination methods.
        </p>
        <p>
        Most of the times, I follow the iterative process of probabilistic modeling: define a model -> use data to infer hidden quantities -> apply, criticize, and modify the model; and repeat. I work on a diverse range of data, so long as they challenge me to think more deeply about statistical methods. Recently, I have a few works on phylogenetic data.
        </p>
        </td> 
        </tr>
      </table>
 
    <br>
      
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Highlighted Publication</heading>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/vcsmc.png' width="280" height="230">
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="https://arxiv.org/pdf/2106.00075.pdf">
            <papertitle>Variational Combinatorial Sequential Monte Carlo Methods in Bayesian Phylogenetic Inference</papertitle>
        </a>
        <br>
          <a href=http://www.cs.columbia.edu/~amoretti/>Antonio K. Moretti*</a>,
          <strong>Liyi Zhang*</strong>,
          <a href=https://naesseth.github.io/>Christian A. Naesseth</a>,
          Hadiah Venner,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a>,
          <a href="http://www.cs.columbia.edu/~itsik/">Itsik Pe'er</a>,
        <br>
        To appear in <em>Uncertainty in Artificial Intelligence 2021 (UAI). </em><a href="https://github.com/amoretti86/phylo">Code.</a><br>
        </p>
        <p>Bayesian phylogenetic inference is often conducted via local or sequential search over topologies and branch lengths using algorithms such as random-walk Markov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC). However, when MCMC is used for evolutionary parameter learning, convergence requires long runs with inefficient exploration of the state space. We introduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful framework that establishes variational sequential search to learn distributions over intricate combinatorial structures. We then develop nested CSMC, an efficient proposal distribution for CSMC and prove that nested CSMC is an exact approximation to the (intractable) locally optimal proposal. We use nested CSMC to define a second objective, VNCSMC which yields tighter lower bounds than VCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore higher probability spaces than existing methods on a range of tasks.
    </td>
    </tr>
    </table>

<br>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publication</heading>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td valign="top" width="75%">
        <p>
        <a href="Writings/VCSMC-MLCB.pdf">
            <papertitle>Variational Combinatorial Sequential Monte Carlo Methods in Bayesian Phylogenetic Inference</papertitle>
        </a>
        <br>
          <a href=http://www.cs.columbia.edu/~amoretti/>Antonio K. Moretti</a>,
          <strong>Liyi Zhang</strong>,
          <a href="http://www.cs.columbia.edu/~itsik/">Itsik Pe'er</a>,
        <br>
        Machine Learning in Computational Biology 2021 (MLCB). </em><a href="https://github.com/amoretti86/phylo">Code.</a><br>
        </p>
    </td>
    </tr>
    </table>

    <br>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Selected Projects</heading>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/stack-ternary.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
        <papertitle>Model Stacking in Bayesian Phylogenetic Inference</papertitle>
        <br>
        <em>Columbia Statistics - Undergraduate Research Internship with </em><a href="http://www.stat.columbia.edu/~gelman/">Andrew Gelman</a><br>
        </p>
        <p>We develop stacking algorithm for phylogenetic inference by isolating discrete models, using MCMC-based methods in Stan for sampling on continuous parameters, and adopting stacking for model combination. Preliminary results support the hypothesis that stacking tends less to produce spuriously high model posteriors than Bayesian Model Averaging.</p>
      </td>
    </tr>
    </table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/bnn-chart.png' width="280" height="270">
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="Writings/BNN-robustness.pdf">
            <papertitle>Improving Neural Network Robustness with Bayesian Weight Sampling</papertitle>
        </a><br>
        <em><a href="http://www.cs.columbia.edu/~blei/fogm/2020F/index.html">STCS 6701 Foundations of Graphical Models</a> - Final Project. </em><a href="https://github.com/zhang-liyi/bnn-robustness">Code.</a><br>
        </p>
        <p>Deep neural networks are the state-of-the-art for various tasks and benchmarks, but they are often not robust to slight or even negligible input perturbations. On image classification tasks, for example, models with near-perfect accuracy can easily degrade to near-zero accuracy when the input images change by a tiny amount that is invisible to human. We propose to improve models’ robustness to input perturbations by adding diversity in model weights during training with Bayesian neural networks. Our experiments on an image classification benchmark shows that Bayesian neural networks are more robust than non-Bayesian deep neural networks trained with norm-based regularization. We claim that introducing diversity of model weights during model training improves model’s robustness to input perturbations. </p>
      </td>
    </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/svrg.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="Writings/SVRG_report.pdf">
            <papertitle>Stochastic Variance Reduction for Nonconvex Optimization</papertitle>
        </a>
        <br>
        <em><a href="https://www.satyenkale.com/optml-f19/">COMS 4995 Optimization Methods in Machine Learning</a> - Final Project. </em><a href="https://github.com/ryandgoldenberg1/svrg_project">Code.</a><br>
        </p>
        <p>Stochastic Variance Reduced Gradient (SVRG) is one of the popular method proposed to reduce the variance of gradient in the optimization. In this project, we evaluate its result on nonconvex optimization. We study the method without the convex assumption and try to reproduce previous experiment results to verify their claims. However, we can only partially reproduce their result and the advantage against SGD no longer holds when we use deeper network.</p>
      </td>
    </tr>
    </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/pso.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="Writings/PSO.pdf">
            <papertitle>Effectiveness and Robustness of Networks in Particle Swarm Optimization</papertitle>
        </a>
        <br>
        <em>Complex Resilient Intelligent Systems Lab - Undergraduate Research Project </em>  <br>
        </p>
        <p>Particle swarm optimization (PSO) is a nature-inspired computational method, in which a swarm of particles solves a problem, and each particle improves its individual solution through interaction with its neighbors. PSO has been successfully applied in a range of practical problems aiming at searching for the global optimum. However, PSO often suffers from premature convergence to local optima. We seek to investigate how network topologies influence its performance. Furthermore, we explore the robustness of these network topologies by incorporating the removal rate of the particles: with a given possibility, particles are getting removed from the game after a fixed number of iterations. In that case, even if some topologies have the same connectivity, they yield different performances. We show the optimization game on several different functions and discuss these factors’ influence on the efficacy and potential of PSO, as well as ideas for further exploration in this area of study.</p>
      </td>
    </tr>
    </table>


    <p> &nbsp </p>
    <p> &nbsp </p>  
  </body>
</html>
