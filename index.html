<html>
<head>
  
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="google-site-verification" content="5XZwYIvqm1A6qR1WOXZFwXynmIvynWzV70s0sRD3yKg" />
  
<style type="text/css">
* {
  margin: 5 px;
  padding: 0;
}
html {
  overflow-y:scroll; /*keep scrollbar position present in FF at all times*/
  height: 100%;
  background-color: ;/* #white;*/
  background-image: ; /* url("fib.jpg") #ffc38b;*/
}
a {
  color: #000099;
}
a:visited {
  color: #000099;
}
a:hover {
  color: #ECCF11;
}
p {
  font-family: "Trebuchet MS", Verdana, Helvetica, sans-serif;
}
p_ {
  font-family: "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
}
blockquote {
    display: block;
    margin-top: 1em;
    margin-bottom: 1em;
    margin-left: 80px;
    margin-right: 80px;
}
table {
    border-style: hidden;
}
li{
  margin: 10px 0;
}
name {
    font-family:  "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
    font-size: 47px;
    font-weight: bold;
}
subtitle {
    font-family:  "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
    font-size: 21px;
    font-weight: bold;
    color: #606060;
}
heading {
    font-family:  "Trebuchet MS", Verdana, Helvetica, sans-serif;
    font-size: 21px;
    font-weight: bold;
}
papertitle {
    font-family:  "Trebuchet MS", Verdana, Helvetica, sans-serif;
    font-size: 18px;
    font-weight: bold;
    }
#friendly_1 {
position:absolute;
left:190px;
top:370px;
width:80px;
}
#text {
position:absolute;
left:19px;
top:270px;
width:80px;
}
#footer {
   width:960px;
   margin:0 auto;
}
#footer p {
   line-height:50px; /* must be same as the amount of padding applied to the bottom of the body so text will center vertically */
   text-align:left; /* align right, left or center */
}
</style>


  <title>Liyi Zhang's Page </title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <p> &nbsp </p>
  </head>
  <body>
  <table width="70%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <p>
          <name>Liyi Zhang</name>
        </p>
        <p>
          <subtitle>M.S. in Data Science at Columbia University</subtitle>
        </p>
        <p>
        <img src="zhang-liyi.jpg" width="25%">
        </p>
        <p>
          <p_>
          <a href="https://github.com/zhang-liyi"> GitHub</a> | 
          <a href="https://twitter.com/LiyiZhang_Leo"> Twitter</a> | 
          <a href="https://www.linkedin.com/in/liyi-zhang-1b5041139/"> LinkedIn</a> | 
          <a> Email:</a>  zhang.liyi@columbia.edu 
          </p_>
        </p>
        </td>
       </tr>
      </table>
      
      <br>
      <br>
      
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
          <td width="100%" valign="middle">
            <heading>Background and Research Interests</heading>
          </td>
        </tr> 
      </table>
      <br>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
        <td width="100%" valign="middle">
        <p>I am an incoming Computer Science PhD at Princeton University, and did M.S. in Data Science and B.A. with Statistics and Applied Math majors at Columbia University. I am advised by professor <a href="http://www.cs.columbia.edu/~blei/">David Blei</a> during masters, and by professor <a href="http://www.cs.columbia.edu/~itsik/">Itsik Pe'er</a> and professor <a href="http://www.stat.columbia.edu/~gelman/">Andrew Gelman</a> during undergrad.</p>
          <p>I am interested in <b>artificial intelligence</b>, with a focus on <b>probabilistic methods</b> and <b>deep generative models</b>. I aim to bring AI closer to humans – having human-like behaviors or becoming more interpretable. Although deep learning offers expressivity, its reasoning process is inadequately understood. Meanwhile, probabilistic modeling complements this weakness and has become fast and scalable. I work on combining these two areas in order to create expressive models with reliable uncertainty estimation, robustness, and a level of interpretability.</p>
        
        </td> 
        </tr>
      </table>
 
    <br>
    <br>
      
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publications and Preprints</heading>
        </td>
      </tr> 
    </table>
      
    <br>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
      <tr>
        <td width="25%">
          <img src='Images/hsc.png' width="90%">
        </td>
        <td valign="top" width="75%">
          <p>
          <a href="https://arxiv.org/pdf/2202.01841.pdf">
            <papertitle>Transport Score Climbing: Variational Inference Using Forward KL and Adaptive Neural Transport</papertitle>
          </a><br>
            <strong>Liyi Zhang</strong>,
            <a href=https://naesseth.github.io/>Christian A. Naesseth</a>,
            <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a>,
          <br>
          <a href="https://github.com/zhang-liyi/tsc">Code.</a><br>
          </p>
          <p>Variational inference often minimizes the "reverse" Kullbeck-Leibler (KL) KL(q||p) from the approximate distribution q to the posterior p. Recent work studies the "forward" KL KL(p||q), which unlike reverse KL does not lead to variational approximations that underestimate uncertainty. This paper introduces Transport Score Climbing (TSC), a method that optimizes KL(p||q) by using Hamiltonian Monte Carlo (HMC) and a novel adaptive transport map. The transport map improves the trajectory of HMC by acting as a change of variable between the latent variable space and a warped space. TSC uses HMC samples to dynamically train the transport map while optimizing KL(p||q). TSC leverages synergies, where better transport maps lead to better HMC sampling, which then leads to better transport maps. We demonstrate TSC on synthetic and real data. We find that TSC achieves competitive performance when training variational autoencoders on large-scale data.</p>
        </td>
      </tr>
      </table>
    
    <br>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/vcsmc.png' width="100%">
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="https://arxiv.org/pdf/2106.00075.pdf">
            <papertitle>Variational Combinatorial Sequential Monte Carlo Methods in Bayesian Phylogenetic Inference</papertitle>
        </a>
        <br>
          <a href=http://www.cs.columbia.edu/~amoretti/>Antonio K. Moretti*</a>,
          <strong>Liyi Zhang*</strong>,
          <a href=https://naesseth.github.io/>Christian A. Naesseth</a>,
          <a href=https://www.linkedin.com/in/hadiah-venner/>Hadiah Venner</a>,
          <a href="http://www.cs.columbia.edu/~blei/">David M. Blei</a>,
          <a href="http://www.cs.columbia.edu/~itsik/">Itsik Pe'er</a>,
        <br>
        <em>Uncertainty in Artificial Intelligence 2021 (UAI). </em><a href="https://github.com/amoretti86/phylo">Code.</a><br>
        </p>
        <p>Bayesian phylogenetic inference is often conducted via local or sequential search over topologies and branch lengths using algorithms such as random-walk Markov chain Monte Carlo (MCMC) or Combinatorial Sequential Monte Carlo (CSMC). However, when MCMC is used for evolutionary parameter learning, convergence requires long runs with inefficient exploration of the state space. We introduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a powerful framework that establishes variational sequential search to learn distributions over intricate combinatorial structures. We then develop nested CSMC, an efficient proposal distribution for CSMC and prove that nested CSMC is an exact approximation to the (intractable) locally optimal proposal. We use nested CSMC to define a second objective, VNCSMC which yields tighter lower bounds than VCSMC. We show that VCSMC and VNCSMC are computationally efficient and explore higher probability spaces than existing methods on a range of tasks.
        <br>
        <p>
        <a href="Writings/VCSMC-MLCB.pdf">
            <papertitle>Variational Combinatorial Sequential Monte Carlo in Bayesian Phylogenetic Inference</papertitle>
        </a>
        <br>
          <a href=http://www.cs.columbia.edu/~amoretti/>Antonio K. Moretti</a>,
          <strong>Liyi Zhang</strong>,
          <a href="http://www.cs.columbia.edu/~itsik/">Itsik Pe'er</a>,
        <br>
        <em>Machine Learning in Computational Biology 2020 (MLCB). </em><a href="https://github.com/amoretti86/phylo">Code.</a><br>
        </p>
    </td>
    </tr>
    </table>

    <br>
    <br>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Selected Projects</heading>
        </td>
      </tr>
    </table>
    <br>
     <br>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/stack-ternary.png' width="100%">
      </td>
      <td valign="top" width="75%">
        <p>
        <papertitle>Model Stacking in Bayesian Phylogenetic Inference</papertitle>
        <br>
        <em>Columbia Statistics - Undergraduate Summer Research Internship with </em><a href="http://www.stat.columbia.edu/~gelman/">Andrew Gelman</a><br>
        </p>
        <p>We develop stacking algorithm for phylogenetic inference by isolating discrete models, using MCMC-based methods in Stan for sampling on continuous parameters, and adopting stacking for model combination. Preliminary results support the hypothesis that stacking tends less to produce spuriously high model posteriors than Bayesian Model Averaging.</p>
      </td>
    </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/bnn-chart.png' width="100%">
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="Writings/BNN-robustness.pdf">
            <papertitle>Improving Neural Network Robustness with Bayesian Weight Sampling</papertitle>
        </a><br>
        <em><a href="http://www.cs.columbia.edu/~blei/fogm/2020F/index.html">STCS 6701 Foundations of Graphical Models</a> - Final Project. </em><a href="https://github.com/zhang-liyi/bnn-robustness">Code.</a><br>
        </p>
        <p>Deep neural networks are the state-of-the-art for various tasks and benchmarks, but they are often not robust to slight or even negligible input perturbations. On image classification tasks, for example, models with near-perfect accuracy can easily degrade to near-zero accuracy when the input images change by a tiny amount that is invisible to human. We propose to improve models’ robustness against input perturbations by adding diversity in model weights during training with Bayesian neural networks. Our experiments on an image classification benchmark shows that Bayesian neural networks are more robust than non-Bayesian deep neural networks trained with norm-based regularization. We claim that introducing diversity of model weights during model training improves models' robustness against input perturbations. </p>
      </td>
    </tr>
    </table>

    

    <p> &nbsp </p>
    <p> &nbsp </p>  
  </body>
</html>
