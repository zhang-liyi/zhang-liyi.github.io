<html>
<head>

<style type="text/css">
* {
  margin: 5 px;
  padding: 0;
}
html {
  overflow-y:scroll; /*keep scrollbar position present in FF at all times*/
  height: 100%;
  background-color: ;/* #white;*/
  background-image: ; /* url("fib.jpg") #ffc38b;*/
}
a {
  color: #240FDF;
}
a:visited {
  color: #240FDF;
}
a:hover {
  color: #ECCF11;
}
p {
  font-family: "Trebuchet MS", Verdana, Helvetica, sans-serif;
}
blockquote {
    display: block;
    margin-top: 1em;
    margin-bottom: 1em;
    margin-left: 80px;
    margin-right: 80px;
}
table {
    border-style: hidden;
}
li{
  margin: 10px 0;
}
name {
    font-family:  "PT Serif","Georgia","Helvetica Neue",Arial,sans-serif; 
    font-size: 47px;
    font-weight: bold;
}
heading {
    font-family:  "Trebuchet MS", Verdana, Helvetica, sans-serif;
    font-size: 21px;
    font-weight: bold;
}
papertitle {
    font-family:  "Trebuchet MS", Verdana, Helvetica, sans-serif;
    font-size: 18px;
    font-weight: bold;
    }
#friendly_1 {
position:absolute;
left:190px;
top:370px;
width:80px;
}
#text {
position:absolute;
left:19px;
top:270px;
width:80px;
}
#footer {
   width:960px;
   margin:0 auto;
}
#footer p {
   line-height:50px; /* must be same as the amount of padding applied to the bottom of the body so text will center vertically */
   text-align:left; /* align right, left or center */
}
</style>


  <title>Liyi Zhang's Page </title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <p> &nbsp </p>
  </head>
  <body>
  <table width="70%" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
        <p>
          <name>Liyi Zhang</name>
        </p>
        <p>
        <img src="zhang-liyi.jpg" width="30%">
        </p>
        <p>
          <a href="Zhang_Liyi_Resume.pdf">Resume</a> | 
          <a href="https://www.linkedin.com/in/liyi-zhang-1b5041139/"> LinkedIn</a> | 
          <a> Email:</a>  lz2574@columbia.edu 
        </p>
        </td>
       </tr>
      </table>
  
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
        <tr>
        <td width="100%" valign="middle">
        <p>
        I'm doing undergraduate degree at Columbia University, Columbia College' 2021. I'm majoring in Statistics and Applied Mathematics. My research interest lies in the general area of probabilistic Machine Learning. They include the intersection between Deep Learning and probabilistic modeling, and approximate Bayesian inference. I'm also interested in model combination methods.
        </p>
        <p>
        Most of the times, I follow the iterative process of probabilistic modeling: define a model -> use data to infer hidden quantities -> apply, criticize, and modify the model; and repeat. I work on a diverse range of data, so long as they challenge me to think more deeply about statistical methods. Recently, I have a few works on phylogenetic data.
        </p>
        </td> 
        </tr>
      </table>
 
    <br>
      google-site-verification=CcWDGVnWpxKIlNK1shuQfNdNdj-atJncaYygDirmABk

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Publication</heading>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/vcsmc-loglik.png' width="280" height="230">
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="Writings/VCSMC-MLCB.pdf">
            <papertitle>Variational Combinatorial Sequential Monte Carlo in Bayesian Phylogenetic Inference</papertitle>
        </a>
        <br>
          <a href=http://www.cs.columbia.edu/~amoretti/>Antonio K. Moretti</a>,
          <strong>Liyi Zhang</strong>,
          <a href="http://www.cs.columbia.edu/~itsik/">Itsik Pe'er</a>,
        <br>
        <em>Machine Learning in Computational Biology 2020 (MLCB). Oral Presentation. </em><a href="https://github.com/amoretti86/phylo">Code.</a><br>
        </p>
        <p>Bayesian phylogenetic inference is often conducted via local or sequential search algorithms such as random-walk Markov chain Monte Carlo or Combinatorial Sequential Monte Carlo. These methods sample tree topologies and branch lengths, however when MCMC is used to perform evolutionary parameter learning, convergence often requires long runs with inefficient state space exploration. We introduce Variational Combinatorial Sequential Monte Carlo (VCSMC), a novel Variational Inference method that simultaneously performs both parameter inference and model learning. Our method uses sequential search to construct a variational objective defined on the composite space of phylogenetic trees. We show that VCSMC is computationally efficient and explores higher probability spaces when compared with state-of-the-art Hamiltonian Monte Carlo methods. </p>
      </td>
    </tr>
    </table>

    <br>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="100%" valign="middle">
          <heading>Selected Projects</heading>
        </td>
      </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/stack-ternary.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
        <papertitle>Model Stacking in Bayesian Phylogenetic Inference</papertitle>
        <br>
        <em>Columbia Statistics - Undergraduate Research Internship with </em><a href="http://www.stat.columbia.edu/~gelman/">Andrew Gelman</a><br>
        </p>
        <p>We develop stacking algorithm for phylogenetic inference by isolating discrete models, using MCMC-based methods in Stan for sampling on continuous parameters, and adopting stacking for model combination. Preliminary results support the hypothesis that stacking tends less to produce spuriously high model posteriors than Bayesian Model Averaging.</p>
      </td>
    </tr>
    </table>
    
    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/bnn-chart.png' width="280" height="270">
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="Writings/BNN-robustness.pdf">
            <papertitle>Improving Neural Network Robustness with Bayesian Weight Sampling</papertitle>
        </a><br>
        <em><a href="http://www.cs.columbia.edu/~blei/fogm/2020F/index.html">STCS 6701 Foundations of Graphical Models</a> - Final Project. </em><a href="https://github.com/zhang-liyi/bnn-robustness">Code.</a><br>
        </p>
        <p>Deep neural networks are the state-of-the-art for various tasks and benchmarks, but they are often not robust to slight or even negligible input perturbations. On image classification tasks, for example, models with near-perfect accuracy can easily degrade to near-zero accuracy when the input images change by a tiny amount that is invisible to human. We propose to improve models’ robustness to input perturbations by adding diversity in model weights during training with Bayesian neural networks. Our experiments on an image classification benchmark shows that Bayesian neural networks are more robust than non-Bayesian deep neural networks trained with norm-based regularization. We claim that introducing diversity of model weights during model training improves model’s robustness to input perturbations. </p>
      </td>
    </tr>
    </table>

    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/svrg.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="Writings/SVRG_report.pdf">
            <papertitle>Stochastic Variance Reduction for Nonconvex Optimization</papertitle>
        </a>
        <br>
        <em><a href="https://www.satyenkale.com/optml-f19/">COMS 4995 Optimization Methods in Machine Learning</a> - Final Project. </em><a href="https://github.com/ryandgoldenberg1/svrg_project">Code.</a><br>
        </p>
        <p>Stochastic Variance Reduced Gradient (SVRG) is one of the popular method proposed to reduce the variance of gradient in the optimization. In this project, we evaluate its result on nonconvex optimization. We study the method without the convex assumption and try to reproduce previous experiment results to verify their claims. However, we can only partially reproduce their result and the advantage against SGD no longer holds when we use deeper network.</p>
      </td>
    </tr>
    </table>


    <table width="100%" align="center" border="0" cellspacing="0" cellpadding="25">
    <tr>
      <td width="25%">
        <img src='Images/pso.png' width="280" height="250">
      </td>
      <td valign="top" width="75%">
        <p>
        <a href="Writings/PSO.pdf">
            <papertitle>Effectiveness and Robustness of Networks in Particle Swarm Optimization</papertitle>
        </a>
        <br>
        <em>Complex Resilient Intelligent Systems Lab - Undergraduate Research Project </em>  <br>
        </p>
        <p>Particle swarm optimization (PSO) is a nature-inspired computational method, in which a swarm of particles solves a problem, and each particle improves its individual solution through interaction with its neighbors. PSO has been successfully applied in a range of practical problems aiming at searching for the global optimum. However, PSO often suffers from premature convergence to local optima. We seek to investigate how network topologies influence its performance. Furthermore, we explore the robustness of these network topologies by incorporating the removal rate of the particles: with a given possibility, particles are getting removed from the game after a fixed number of iterations. In that case, even if some topologies have the same connectivity, they yield different performances. We show the optimization game on several different functions and discuss these factors’ influence on the efficacy and potential of PSO, as well as ideas for further exploration in this area of study.</p>
      </td>
    </tr>
    </table>


    <p> &nbsp </p>
    <p> &nbsp </p>  
  </body>
</html>
